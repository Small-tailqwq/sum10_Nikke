import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from sklearn.model_selection import train_test_split
import numpy as np

# === é…ç½® ===
DATASET_DIR = 'dataset'
MODEL_PATH = 'sum10_model.pth'
IMG_SIZE = 64
BATCH_SIZE = 8
EPOCHS = 100     # è¿˜æ˜¯100è½®ï¼Œç¡®ä¿æ”¶æ•›
LEARNING_RATE = 0.001

# === æ¨¡å‹å®šä¹‰ (ä¿æŒä¸å˜) ===
class SimpleDigitNet(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleDigitNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# === æ•°æ®åŠ è½½ ===
class DigitDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    def __len__(self): return len(self.image_paths)
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('L')
        label = self.labels[idx]
        if self.transform: image = self.transform(image)
        return image, label

def load_data():
    image_paths = []
    labels = []
    # å¼ºåˆ¶é‡æ–°æ‰«æï¼Œç¡®ä¿è¯»å–æ‰€æœ‰æ•°æ®
    classes = sorted([d for d in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, d))])
    class_to_idx = {c: int(c) for c in classes if c.isdigit()}
    
    print(f"æ ‡ç­¾æ˜ å°„: {class_to_idx}")
    
    for class_name, class_idx in class_to_idx.items():
        folder_path = os.path.join(DATASET_DIR, class_name)
        for fname in os.listdir(folder_path):
            if fname.lower().endswith(('.png', '.jpg')):
                image_paths.append(os.path.join(folder_path, fname))
                labels.append(class_idx)
    return image_paths, labels

# === è®­ç»ƒæµç¨‹ ===
def train():
    img_paths, labels = load_data()
    if not img_paths: return

    # åˆ’åˆ†æ•°æ®é›†
    train_paths, val_paths, train_labels, val_labels = train_test_split(
        img_paths, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # æ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)), #ç¨å¾®åŠ å¤§ä¸€ç‚¹éš¾åº¦
        transforms.ToTensor(),
    ])
    val_transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(DigitDataset(train_paths, train_labels, train_transform), batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(DigitDataset(val_paths, val_labels, val_transform), batch_size=BATCH_SIZE, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Device: {device}")
    
    model = SimpleDigitNet(num_classes=10).to(device)
    
    # ==========================================
    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šç±»åˆ«æƒé‡ (Class Weights)
    # ==========================================
    # ç»™ 4 å’Œ 8 æ›´é«˜çš„æƒé‡ï¼Œé€¼è¿«æ¨¡å‹åŒºåˆ†å®ƒä»¬
    class_weights = torch.ones(10).to(device)
    class_weights[4] = 3.0  # è®¤é”™ 4 çš„æƒ©ç½šæ˜¯å¹³æ—¶çš„3å€
    class_weights[8] = 3.0  # è®¤é”™ 8 çš„æƒ©ç½šæ˜¯å¹³æ—¶çš„3å€
    
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, factor=0.5)

    best_acc = 0.0
    print("\nğŸ”¥ å¼€å§‹å¸¦æƒé‡çš„å¼ºåŒ–è®­ç»ƒ...")

    for epoch in range(EPOCHS):
        model.train()
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        acc = 100 * correct / total
        scheduler.step(acc)

        if acc >= best_acc: # åªæœ‰æ›´å¥½æˆ–æŒå¹³æ—¶æ‰ä¿å­˜
            best_acc = acc
            torch.save(model.state_dict(), MODEL_PATH)
            
        if (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}/{EPOCHS} | Val Acc: {acc:.2f}% (Best: {best_acc:.2f}%)")

    print(f"âœ… è®­ç»ƒç»“æŸã€‚æ¨¡å‹å·²ä¿å­˜ã€‚æœ€ä½³å‡†ç¡®ç‡: {best_acc}%")

if __name__ == '__main__':
    train()